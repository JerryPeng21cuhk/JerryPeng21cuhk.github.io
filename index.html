<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Zhiyuan(Jerry) Peng(incoming NCSU post-doc)</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center" lang="zh-cn"><name>Zhiyuan PENG 彭志远</name></p>
              <p> I am an incoming Post-doc in the Reliable & Efficient Computing Lab led by <a href="https://dongkuanx27.github.io/">DK Xu</a> and the <a href="http://www.nicelab.us/NICE-LAB/">NICE lab</a> led by <a href="http://www.nicelab.us/">Yuchen Liu</a> at <a href="https://www.ncsu.edu/">North Carolina State University</a>. My recent research interest mainly focuses on improving the efficiency and reliablity of augmented language models (ALMs) and applying intelligent agents (powered by ALMs) into networking. 
              </p>
              <p>
              Before coming to NC State, I obtained my Ph.D degree of Electronic Engineering at <a href="https://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong (CUHK)</a> advised by <a href="http://www.ee.cuhk.edu.hk/~tanlee/my_students.html">Tan LEE</a>, focused on probabilistic speaker modeling for speech processing. 
              </p>

              <p align=center>
                <a href="mailto:jerrypeng1937@gmail.com">Email</a> &nbsp/&nbsp
                <a href="resume/My_resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=j6gmTmsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/JerryPeng21cuhk">GitHub</a> 
              </p>
            </td>
            <td width="30%">
              <img src="image/jerry.jpeg" height="260" width="260">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
              	
              </p>
              <p>
              I am now devoted to efficient prompting paradigms for tool-augmented large language models (ALMs). I'm also interested in other topics relevant to ALMs, including tool management and optimization, acceleration and automation of ALMs, etc. We have recently released <a href="https://github.com/billxbf/ReWOO">ReWOO</a>, a work on <a href="https://arxiv.org/pdf/2305.18323.pdf">efficient prompting</a>.
              </p>
            </td>
          </tr>
        </table>
        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
                <heading>Selected Publications<a href="https://scholar.google.com/citations?user=j6gmTmsAAAAJ">[Google Scholar]</a></heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/rewoo.png" alt="pacman" width="180" height="180"></td>
            <td width="75%" valign="center">
            <p>
                <papertitle>
                <a href="https://arxiv.org/abs/2305.18323.pdf">ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</a>
            </p>
            <p>
            Binfeng Xu, <b>Zhiyuan Peng</b>, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, Dongkuan Xu, submitted to <b>NeurIPS</b>
            </p>
            <p>
            Existing ALM systems trigger LLM thought processes while pulling observations from external tools in an interleaved fashion. Such paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This paper addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption.
            </p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/splda.png" alt="pacman" width="180" height="180"></td>
            <td width="75%" valign="center">
            <p>
                <papertitle>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094608">Covariance regularization for Probabilistic Linear Discriminant Analysis </a>
            </p>
            <p>
            
            <b>Zhiyuan Peng</b>, Mingjie Shao, Xuanji He, Xu Li, Tan Lee, Ke Ding, Guanglu Wan <b>ICASSP
            </b>
            </p>
            <p>
            Probabilistic linear discriminant analysis (PLDA) is commonly used in speaker verification systems to score the similarity of speaker embeddings. Recent studies improved the performance of PLDA in domain-matched conditions by diagonalizing its covariance. We suspect such a brutal pruning approach could eliminate its capacity in modeling dimension correlation of speaker embeddings, leading to inadequate performance with domain adaptation. This paper explores two alternative covariance regularization approaches, namely, interpolated PLDA and sparse PLDA, to tackle the problem.
            </p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/unify.png" alt="pacman" width="180" height="180"></td>
            <td width="75%" valign="center">
            <p>
                <papertitle>
                <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/peng22b_interspeech.pdf">Unifying Cosine and PLDA Back-ends for Speaker Verification</a>
            </p>
            <p>
            <b>Zhiyuan Peng</b>, Xuanji He, Ke Ding, Tan Lee, Guanglu Wan, <b>Interspeech</b>
            </b>
            </p>
            <p>
            State-of-art speaker verification (SV) systems use a backend model to score the similarity of speaker embeddings extracted from a neural network. The commonly used back-ends are the cosine scoring and the probabilistic linear discriminant analysis (PLDA) scoring. With the recently developed neural embeddings, the theoretically more appealing PLDA approach is found to have no advantage against or even be inferior to the simple cosine scoring in terms of verification performance. This paper presents an investigation on the relation between the two back-ends, aiming to explain the above counter-intuitive observation. It is shown that the cosine scoring is essentially a special case of PLDA scoring.
            </p>
            </td>
          </tr>
        </table>
        
        
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/distill.png" alt="pacman" width="180" height="180"></td>
            <td width="150%" valign="center">
            <p>
                <papertitle>
                <a href="https://arxiv.org/abs/2212.03090">Label-free Knowledge Distillation with Contrastive Loss for Light-weight Speaker Recognition</a>
            </p>
            <p>
            
            <b>Zhiyuan Peng</b>, Xuanji He, Ke Ding, Tan Lee, Guanglu Wan, <b>ISCSLP</b>
            </p>
            <p>
            Very deep models for speaker recognition (SR) have demonstrated remarkable performance improvement in recent research. However, it is impractical to deploy these models for on-device applications with constrained computational resources. On the other hand, light-weight models are highly desired in practice despite their sub-optimal performance. This research aims to improve light-weight SR models through large-scale label-free knowledge distillation (KD). Existing KD approaches for SR typically require speaker labels to learn task-specific knowledge, due to the inefficiency of conventional loss for distillation. To address the inefficiency problem and achieve label-free KD, we propose to employ the contrastive loss from self-supervised learning for distillation.
            </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/twin.png" alt="pacman" width="180" height="180"></td>
            <td width="150%" valign="center">
            <p>
                <papertitle>
                <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/peng21d_interspeech.pdf">Pairing Weak with Strong: Twin Models for Defending against Adversarial Attack on Speaker Verification</a>
            </p>
            <p>
            
            <b>Zhiyuan Peng</b>, Xu Li, Tan Lee, <b>Interspeech</b>
            </p>
            <p>
            Vulnerability of speaker verification (SV) systems under adversarial attack receives wide attention recently. Simple and effective countermeasures against such attack are yet to be developed. This paper formulates the task of adversarial defense as a problem of attack detection. The detection is made possible with the verification scores from a pair of purposely selected SV models. The twin-model design comprises a fragile model paired up with a relatively robust one. The two models show prominent score inconsistency under adversarial attack. To detect the score inconsistency, a simple one-class classifier is adopted. The classifier is trained with normal speech samples, which not only bypasses the need of crafting adversarial samples but also prevents itself from over-fitting to the crafted samples, and hence makes the detection robust to unseen attacks. Compared to single-model systems, the proposed system shows consistent and significant performance improvement against different attack strategies.
            </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/mfae.png" alt="pacman" width="180" height="180"></td>
            <td width="150%" valign="center">
            <p>
                <papertitle>
                <a href="https://arxiv.org/abs/1911.01806">Mixture factorized auto-encoder for unsupervised hierarchical deep factorization of speech signal</a>
            </p>
            <p>
            
            <b>Zhiyuan Peng</b>, Siyuan Feng, Tan Lee, <b>ICASSP</b>
            </p>
            <p>
            Speech signal is constituted and contributed by various informative factors, such as linguistic content and speaker characteristic. There have been notable recent studies attempting to factorize speech signal into these individual factors without requiring any annotation. These studies typically assume continuous representation for linguistic content, which is not in accordance with general linguistic knowledge and may make the extraction of speaker information less successful. This paper proposes the mixture factorized auto-encoder (mFAE) for unsupervised deep factorization. The encoder part of mFAE comprises a frame tokenizer and an utterance embedder. The frame tokenizer models linguistic content of input speech with a discrete categorical distribution. It performs frame clustering by assigning each frame a soft mixture label. The utterance embedder generates an utterance-level vector representation. A frame decoder serves to reconstruct speech features from the encoders' outputs. The mFAE is evaluated on speaker verification (SV) task and unsupervised subword modeling (USM) task.
            </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/lid.png" alt="pacman" width="180" height="180"></td>
            <td width="150%" valign="center">
            <p>
                <papertitle>
                <a href="https://ieeexplore.ieee.org/document/8682303">Adversarial multi-task deep features and unsupervised back-end adaptation for language recognition</a>
            </p>
            <p>
            
            <b>Zhiyuan Peng</b>, Siyuan Feng, Tan Lee, <b>ICASSP</b>
            </p>
            <p>
            This paper presents an investigation into speaker-invariant feature learning and domain adaptation for language recognition (LR) with short utterances. While following the conventional design of i-vector front-end and probabilistic linear discriminant analysis (PLDA) back-end, we propose to apply speaker adversarial multi-task learning (AMTL) to aim explicitly at learning speaker-invariant multilingual bottleneck features and perform unsupervised PLDA adaptation to alleviate performance degradation caused by domain mismatch between training and test data.
            </p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/meituan.png" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <papertitle>Research Intern in Speech Tech. Group in Meituan, Aug.2021-May.2022 (Beijing, China)</papertitle>
                <br>
              </p>
              <p>
                Developed back-end techniques with recent advancements in speaker verification (Bayesian probabilistic modeling for PLDA
              </p>
              <p>
                Experimented wav2vec2 for the unsupervised model pretraining of speech recognition 
              </p>

            </td>
          </tr>
        </table>   
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Courses taken</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/pic1.jpeg" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
            <p>
                Introduction to Deep learning - Spring 2019 (CUHK)
                <br>
                      
            </p>
            <p>
                Speech and Language Processing - Spring 2018 (CUHK)
                <br>
                  
            </p>
              <p>
                Probabilistic Models and Inference Algorithms for Machine Learning - Fall 2018 (CUHK)
                <br>
                
              </p>
              <p>
                Big Data Analytics - Fall 2020 (CUHK)
                <br>
              </p>
              <p>
                C, C++, FPGA, Networking, Matlab, Circuit Design - Undergraduate
                <br>                
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Professional Service</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/peer_review.jpeg" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <papertitle>Speech Communication, ICASSP 2022, Interspeech 2023 (Reviewer)</papertitle>
                <br>
              </p>

            </td>
          </tr>
        </table>        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Assistantship Experience</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="image/teaching.png" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
              <papertitle>Teaching Assistant - C++ language (<a href="https://www.hkage.edu.hk/">HKAGE</a>) - Fall 2021</papertitle>
                <br>
                
              </p>
              <p>
                <papertitle>Graduate Teaching Assistant for Digital Signal Processing (design project with FPGA for adaptive noise cancelling)  - From Fall 2017 to Spring 2021 (CUHK), won best tutor award</papertitle>
                <br>
                
              </p>
            </td>
          </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="center">
          <!-- Start of CuterCounter Code -->
          <a href="http://www.cutercounter.com/" target="_blank"><img src="https://www.cutercounter.com/hits.php?id=huxnnofd&nd=4&style=1" border="0" alt="free counter">
           <!-- End of CuterCounter Code --> 
          </a>
        </p>
        </td>
      </tr>
      </table>  

      
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      </td>
      </tr>
      </table>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
